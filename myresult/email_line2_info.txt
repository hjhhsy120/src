Reading...
Pre-procesing for non-uniform negative sampling!
For positive: data size = 32770
Small prob: 0
epoch:0 sum of loss:4348.86740813
Training classifier using 50.00% nodes...
{'micro': 0.095427435387673953, 'macro': 0.0073838493476132286, 'samples': 0.095427435387673953, 'weighted': 0.035334415060035623}
epoch:1 sum of loss:2809.82186386
Training classifier using 50.00% nodes...
{'micro': 0.11332007952286283, 'macro': 0.019455486339484913, 'samples': 0.11332007952286283, 'weighted': 0.050062626922783494}
epoch:2 sum of loss:2342.89829522
Training classifier using 50.00% nodes...
{'micro': 0.1172962226640159, 'macro': 0.015318169637931306, 'samples': 0.1172962226640159, 'weighted': 0.048362061987558269}
epoch:3 sum of loss:1934.97037256
Training classifier using 50.00% nodes...
{'micro': 0.1172962226640159, 'macro': 0.010203227173681014, 'samples': 0.1172962226640159, 'weighted': 0.045787890592042461}
epoch:4 sum of loss:1621.85610497
Training classifier using 50.00% nodes...
{'micro': 0.12922465208747516, 'macro': 0.012002720375361885, 'samples': 0.12922465208747516, 'weighted': 0.052564018405291527}
72.73042583465576
Saving embeddings...
Training classifier using 50.00% nodes...
{'micro': 0.11332007952286283, 'macro': 0.019455486339484913, 'samples': 0.11332007952286283, 'weighted': 0.050062626922783494}
